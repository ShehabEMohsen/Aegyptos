{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed B\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1072, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded_model = pickle.load(\"squeezenetfinetuned.pkl\")\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "#Basma\n",
    "# classifer = joblib.load(\"C:/Users/basma/Fourth Year/Graduation_Project/squeezenetfinetuned.pkl\")\n",
    "\n",
    "#Bassem\n",
    "# classifer = joblib.load(\"squeezenetuntrained.pkl\")\n",
    "\n",
    "model = pickle.load(open('new squeezenet(finetuned).pkl', 'rb'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "deleteCounterImage = 0\n",
    "\n",
    "# Set threshold values for binary image and blob analysis\n",
    "binary_threshold = 50\n",
    "blob_min_area = 100\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imread(\"experiments/bad_evil_snake.png\")\n",
    "\n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Otsu's thresholding method to get a binary image\n",
    "_, threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Invert the binary image\n",
    "binary = cv2.bitwise_not(threshold)\n",
    "\n",
    "# Perform blob analysis\n",
    "contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Sort contours by x-coordinate and then by y-coordinate\n",
    "# contours = sorted(contours, key=lambda c: (cv2.boundingRect(c)[0], cv2.boundingRect(c)[1]))\n",
    "# Sort contours from top to bottom and left to right\n",
    "contours = sorted(contours, key=lambda c: (cv2.boundingRect(c)[1], cv2.boundingRect(c)[0]))\n",
    "\n",
    "# Iterate over the sorted contours\n",
    "for i, contour in enumerate(contours):\n",
    "    \n",
    "    # Check if the area of the contour is large enough to be considered an object\n",
    "    if cv2.contourArea(contour) > blob_min_area:\n",
    "        # Get bounding box coordinates\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Crop the image using the bounding box coordinates\n",
    "        crop_img = image[y:y+h, x:x+w]\n",
    "        # Save the cropped image to a file\n",
    "        filename = f\"crop_{i}.png\"\n",
    "        cv2.imwrite(filename, crop_img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana:  0\n",
      "ana:  1\n",
      "ana:  2\n",
      "ana:  3\n",
      "ana:  4\n",
      "ana:  5\n",
      "ana:  6\n",
      "ana:  7\n",
      "ana:  8\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import glob\n",
    "\n",
    "image_array = []\n",
    "image_files = glob.glob(\"*crop*.png\")\n",
    "for i, image_file in enumerate(image_files):\n",
    "    \n",
    "    print(\"ana: \",i)\n",
    "    # Load the input image\n",
    "    image1 = cv2.imread(image_file)\n",
    "\n",
    "    # Get the original dimensions of the image\n",
    "    height, width, _ = image1.shape\n",
    "\n",
    "    # Calculate the aspect ratio of the image\n",
    "    aspect_ratio = width / height\n",
    "\n",
    "    # Set the desired output size (height or width)\n",
    "    output_size = 224\n",
    "\n",
    "    # Calculate the other dimension based on the aspect ratio\n",
    "    if width > height:\n",
    "        new_width = output_size\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = output_size\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image1, (new_width, new_height))\n",
    "\n",
    "    # Calculate the coordinates to copy the resized image onto the new image\n",
    "    x_offset = int((output_size - new_width) / 2)\n",
    "    y_offset = int((output_size - new_height) / 2)\n",
    "\n",
    "    # Save the resized image to a file\n",
    "    cv2.imwrite(f\"resize{i}.png\", resized_image)\n",
    "\n",
    "    # Load the resized image from file\n",
    "    resized_image = cv2.imread(f\"resize{i}.png\")\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Otsu's thresholding method to get a binary image\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "    # Invert the binary image\n",
    "    inv = cv2.bitwise_not(thresh)\n",
    "    # Save the segmented image to a file\n",
    "    cv2.imwrite(f\"seg_img{i}.png\", inv)\n",
    "    # Load the segmented image from file\n",
    "    seg_img = cv2.imread(f\"seg_img{i}.png\")\n",
    "\n",
    "    # Create a new blank image with the desired output size\n",
    "    new_image = np.zeros((output_size, output_size, 3), np.uint8)\n",
    "    new_image[:, :] = (0, 0, 0)  # fill with black pixels\n",
    "\n",
    "    # Copy the segmented image onto the new image with black pixels\n",
    "    new_image[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = seg_img\n",
    "\n",
    "    # Save the new image to a file\n",
    "    cv2.imwrite(f\"final{i}.png\", new_image)\n",
    "    deleteCounterImage = deleteCounterImage+1\n",
    "    image_array.append(new_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "tensor([14.3750, 16.7328, 16.2243,  ...,  7.7657,  6.8522,  9.4124])\n",
      "I009 0.35691094398498535\n",
      "D035 0.24751420319080353\n",
      "D030 0.08463063836097717\n",
      "D044 0.0448552705347538\n",
      "V011B 0.038954958319664\n",
      "I009\n",
      "Percentage of confidence for T035: 0.00%\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed B\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.8701, 16.6496, 17.9634,  ..., 15.7820, 11.0325, 10.3741])\n",
      "N035A 0.6072943806648254\n",
      "N035 0.38613319396972656\n",
      "F032 0.006270901765674353\n",
      "NU011 9.653968299971893e-05\n",
      "E002 6.764876161469147e-05\n",
      "N035A\n",
      "Percentage of confidence for T035: 0.00%\n",
      "9\n",
      "tensor([11.4104,  6.3771,  3.9526,  ...,  1.9068,  1.7442,  4.4242])\n",
      "U036 0.621677577495575\n",
      "T035 0.3487195372581482\n",
      "S002 0.013632675632834435\n",
      "AA007B 0.002668979112058878\n",
      "F009 0.0019910582341253757\n",
      "U036\n",
      "Percentage of confidence for T035: 34.87%\n",
      "9\n",
      "tensor([4.9667, 2.5005, 4.1679,  ..., 3.9314, 4.9319, 7.1141])\n",
      "U028 0.12581658363342285\n",
      "AA007B 0.10297230631113052\n",
      "S043 0.09682084619998932\n",
      "X007 0.09406831115484238\n",
      "W004 0.03190876543521881\n",
      "U028\n",
      "Percentage of confidence for T035: 0.07%\n",
      "9\n",
      "tensor([17.5514, 25.4380, 18.7231,  ..., 11.3540,  7.8485, 11.1756])\n",
      "I010 0.8909083604812622\n",
      "I011 0.06865179538726807\n",
      "V012B 0.027964770793914795\n",
      "V011C 0.003634243505075574\n",
      "F020 0.0033053955994546413\n",
      "I010\n",
      "Percentage of confidence for T035: 0.00%\n",
      "9\n",
      "tensor([12.3210, 23.7244, 21.3649,  ..., 17.9205, 10.9830, 13.4023])\n",
      "I014 0.9986382126808167\n",
      "I015 0.0013564358232542872\n",
      "T012 2.7490229967952473e-06\n",
      "Z004A 1.1117908798041753e-06\n",
      "O003 4.466547522952169e-07\n",
      "I014\n",
      "Percentage of confidence for T035: 0.00%\n",
      "9\n",
      "tensor([14.6282, 17.1244, 18.4282,  ...,  9.1606, 10.4300, 12.9553])\n",
      "G037 0.9132487177848816\n",
      "D030 0.02951071783900261\n",
      "G051 0.016385072842240334\n",
      "G031 0.009656495414674282\n",
      "E038 0.006555482279509306\n",
      "G037\n",
      "Percentage of confidence for T035: 0.00%\n",
      "9\n",
      "tensor([11.2038, 16.7295,  6.2250,  ...,  7.6378,  3.8299,  8.6381])\n",
      "AA007 0.17313115298748016\n",
      "V023 0.09732267260551453\n",
      "M010A 0.06849987804889679\n",
      "T026 0.06642605364322662\n",
      "I010 0.05823796987533569\n",
      "AA007\n",
      "Percentage of confidence for T035: 0.00%\n",
      "9\n",
      "tensor([16.9395, 14.6818, 12.9443,  ..., 13.9102, 11.4246, 10.0454])\n",
      "D046 0.9348812103271484\n",
      "D046A 0.05603090301156044\n",
      "O034 0.0010716670658439398\n",
      "D030 0.0010602085385471582\n",
      "V011B 0.0008979220292530954\n",
      "D046\n",
      "Percentage of confidence for T035: 0.00%\n",
      "I009 N035A U036 U028 I010 I014 G037 AA007 D046\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "import cv2\n",
    "\n",
    "results = \"\"\n",
    "#Basma\n",
    "for i in range(deleteCounterImage):\n",
    "    print(deleteCounterImage)\n",
    "    input_image = Image.open(f'final{i}.png').convert('RGB')\n",
    "    # input_image = Image.open('C:/Users/basma/Fourth Year/Graduation_Project/SegmentedDatasetAgain/train/A040/A040_1.png').convert('RGB')\n",
    "    #Bassem\n",
    "    # input_image = Image.open('D:/Ahmed Bassem/MIU/Year 4/Graduation Project/Datasets/SegmentedDataset/SegmentedDataset/test/O001/O001_1.png').convert('RGB')\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        # transforms.Pad(50),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    # input_tensor = preprocess(Image.fromarray(i))\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    # Tensor of shape 1000, with confidence scores over Aegyptos' 1072 classes\n",
    "    print(output[0])\n",
    "    # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "    # Read the categories\n",
    "    #Basma\n",
    "    with open(\"classes_names.txt\", \"r\") as f:\n",
    "\n",
    "    #Bassem\n",
    "    # with open(\"D:/Ahmed Bassem/MIU\\Year 4/Graduation Project/classes_names.txt\", \"r\") as f:\n",
    "        categories = [s.strip() for s in f.readlines()]\n",
    "    # Show top categories per image\n",
    "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "    for i in range(top5_prob.size(0)):\n",
    "        print(categories[top5_catid[i]], top5_prob[i].item())\n",
    "\n",
    "    top1 = categories[top5_catid[0]]\n",
    "    print(top1)\n",
    "\n",
    "    class_name = 'T035'\n",
    "    class_index = categories.index(class_name)\n",
    "    class_prob = probabilities[class_index] * 100\n",
    "    print(f\"Percentage of confidence for {class_name}: {class_prob:.2f}%\")\n",
    "\n",
    "    # results = results + top1 + \" \"\n",
    "    results += \" \"+ top1\n",
    "updated_results = results.replace(\" \", \"\", 1)\n",
    "    # results.append(top1)\n",
    "print(updated_results)\n",
    "choose_dictionary_list=deleteCounterImage-1\n",
    "print(choose_dictionary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# for i in range(deleteCounterImage):\n",
    "#     print(i)\n",
    "#     if(updated_results[i] == 'A'):\n",
    "#         print(\"A\")\n",
    "#         print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake\n"
     ]
    }
   ],
   "source": [
    "# import matching_algorithm as ma\n",
    "# print(ma.dictionary_matching(updated_results))\n",
    "\n",
    "##NEW CODE\n",
    "import matching_algorithm3 as ma\n",
    "print(ma.dictionary_matching(updated_results,choose_dictionary_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete previously created images\n",
    "cropImageRemovals = glob.glob(\"*crop*.png\")\n",
    "for i in range(deleteCounterImage):\n",
    "    removeCrop = cropImageRemovals\n",
    "    removeSegImg = f\"seg_img{i}.png\"\n",
    "    removeFinal = f\"final{i}.png\"\n",
    "    removeResize = f\"resize{i}.png\"\n",
    "    try:\n",
    "        os.remove(removeCrop[i])\n",
    "        os.remove(removeSegImg)\n",
    "        os.remove(removeFinal)\n",
    "        os.remove(removeResize)\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لا يوجد صوت\n"
     ]
    }
   ],
   "source": [
    "import matching_algorithm_pronunciation as ma\n",
    "print(ma.dictionary_matching_pronunciation(updated_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans\n",
    "from googletrans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matching_algorithm_pronunciation' has no attribute 'dictionary_matching'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Ahmed Bassem\\MIU\\Year 4\\Graduation Project\\Flask Code\\Aegyptos\\predictionscropped.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Ahmed%20Bassem/MIU/Year%204/Graduation%20Project/Flask%20Code/Aegyptos/predictionscropped.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m match \u001b[39m=\u001b[39m ma\u001b[39m.\u001b[39;49mdictionary_matching(updated_results)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matching_algorithm_pronunciation' has no attribute 'dictionary_matching'"
     ]
    }
   ],
   "source": [
    "match = ma.dictionary_matching(updated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'match' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Ahmed Bassem\\MIU\\Year 4\\Graduation Project\\Flask Code\\Aegyptos\\predictionscropped.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Ahmed%20Bassem/MIU/Year%204/Graduation%20Project/Flask%20Code/Aegyptos/predictionscropped.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m translator \u001b[39m=\u001b[39m googletrans\u001b[39m.\u001b[39mTranslator()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Ahmed%20Bassem/MIU/Year%204/Graduation%20Project/Flask%20Code/Aegyptos/predictionscropped.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m translate\u001b[39m=\u001b[39mtranslator\u001b[39m.\u001b[39mtranslate(match,dest\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39marabic\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Ahmed%20Bassem/MIU/Year%204/Graduation%20Project/Flask%20Code/Aegyptos/predictionscropped.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(translate\u001b[39m.\u001b[39mtext)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'match' is not defined"
     ]
    }
   ],
   "source": [
    "translator = googletrans.Translator()\n",
    "translate=translator.translate(match,dest='arabic')\n",
    "print(translate.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
