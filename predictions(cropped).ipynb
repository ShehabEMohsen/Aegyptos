{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1072, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded_model = pickle.load(\"squeezenetfinetuned.pkl\")\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "#Basma\n",
    "# classifer = joblib.load(\"C:/Users/basma/Fourth Year/Graduation_Project/squeezenetfinetuned.pkl\")\n",
    "\n",
    "#Bassem\n",
    "# classifer = joblib.load(\"squeezenetuntrained.pkl\")\n",
    "\n",
    "model = pickle.load(open('new squeezenet(finetuned).pkl', 'rb'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Segmentation then Cropping algorithm \n",
    "# import cv2\n",
    "# from imutils import contours\n",
    "# from skimage.filters import threshold_otsu\n",
    "\n",
    "# # Load image, grayscale, Otsu's threshold\n",
    "# image_array = []\n",
    "# results = []\n",
    "# # image = cv2.imread('C:/Users/basma/Fourth Year/Graduation_Project/O001 (1).png')\n",
    "# # image_array.append(image)\n",
    "# # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# # thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "# # inv = cv2.bitwise_not(thresh)\n",
    "# # rgb = cv2.cvtColor(inv, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# # Load image\n",
    "# image = cv2.imread('walk (1).png')\n",
    "\n",
    "# # Convert image to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Threshold image\n",
    "# _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "# # Find contours\n",
    "# contours,_  = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "# #---------------------------------------------------------------------\n",
    "# # def Otsu(myImage):\n",
    "\n",
    "#   # image = cv2.imread(myImage)\n",
    "#   # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#   # thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "#   # inv = cv2.bitwise_not(thresh)\n",
    "#   # rgb = cv2.cvtColor(inv, cv2.COLOR_GRAY2RGB)\n",
    "#   # # image_array.append(rgb)\n",
    "\n",
    "#   # return inv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   # sample_image = cv2.imread(myImage)\n",
    "#   # img = cv2.cvtColor(sample_image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#   # # plt.axis('off')\n",
    "#   # # plt.imshow(img)\n",
    "\n",
    "#   # img_gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "#   # thresh = threshold_otsu(img_gray)\n",
    "#   # img_otsu  = img_gray < thresh\n",
    "\n",
    "#   # # plt.imshow(img_otsu)\n",
    "\n",
    "#   # filtered = filter_image(img, img_otsu)\n",
    "#   # filteredBW=cv2.cvtColor(filtered,cv2.COLOR_RGB2GRAY)\n",
    "#   # return filteredBW\n",
    "\n",
    "\n",
    "\n",
    "# def filter_image(image, mask):\n",
    "\n",
    "#     r = image[:,:,0] * mask\n",
    "#     g = image[:,:,1] * mask\n",
    "#     b = image[:,:,2] * mask\n",
    "\n",
    "#     return np.dstack([r,g,b])\n",
    "\n",
    "# # image = Otsu('D:/semester 7/Graduation Project/Datasets/new dataset/O001.png')\n",
    "\n",
    "# # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# # thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "# # inv = cv2.bitwise_not(thresh)\n",
    "\n",
    "# cv2.imwrite('image2.png', image)\n",
    "# #---------------------------------------------------------------------\n",
    "\n",
    "# # Find contours, sort from left-to-right, then crop\n",
    "# # cnts = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# # cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "# # cnts, _ = contours.sort_contours(cnts, method=\"left-to-right\")\n",
    "\n",
    "# # ROI_number = 0\n",
    "# # for c in cnts:\n",
    "# #     area = cv2.contourArea(c)\n",
    "# #     if area > 50:\n",
    "# #         x,y,w,h = cv2.boundingRect(c)\n",
    "# #         ROI = 255 - image[y:y+h, x:x+w]\n",
    "# #         # img1 = cv2.copyMakeBorder(ROI, 20,20,20,20,cv2.BORDER_CONSTANT, value=[255,255,255])\n",
    "# #         img2 = cv2.bitwise_not(ROI)\n",
    "# #         cv2.imwrite('ROIVVV_{}.png'.format(ROI_number), img2)\n",
    "        \n",
    "# #         image_array.append(img2)\n",
    "# #         cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "# #         ROI_number += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Crop symbols\n",
    "# for i, contour in enumerate(contours):\n",
    "#     # Get bounding rectangle\n",
    "#     x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "#     # Crop symbol\n",
    "#     symbol = image[y:y+h, x:x+w]\n",
    "\n",
    "#     # image = cv2.imread(symbol)\n",
    "#     gray = cv2.cvtColor(symbol, cv2.COLOR_BGR2GRAY)\n",
    "#     thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "#     inv = cv2.bitwise_not(thresh)\n",
    "\n",
    "#     # Save symbol\n",
    "#     cv2.imwrite(f\"symbol{i}.png\", inv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the YOLOv3 network\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "# Set confidence threshold and non-maximum suppression threshold for YOLO\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imread(\"walk.png\")\n",
    "\n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Threshold image\n",
    "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "# Iterate over the contours\n",
    "for i, contour in enumerate(contours):\n",
    "    # Get bounding box coordinates\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "    # Crop the image using the bounding box coordinates\n",
    "    crop_img = image[y:y+h, x:x+w]\n",
    "\n",
    "    # Create a blob from the cropped image\n",
    "    blob = cv2.dnn.blobFromImage(crop_img, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # Pass the blob through the network to perform object detection\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    # Iterate over the output layers\n",
    "    for output in outputs:\n",
    "        # Iterate over the detected objects\n",
    "        for detection in output:\n",
    "            # Extract the class ID and confidence score\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            # Check if the detected object is above the confidence threshold\n",
    "            if confidence > conf_threshold:\n",
    "                # Compute the bounding box coordinates of the detected object\n",
    "                center_x = int(detection[0] * w)\n",
    "                center_y = int(detection[1] * h)\n",
    "                box_width = int(detection[2] * w)\n",
    "                box_height = int(detection[3] * h)\n",
    "                left = int(center_x - box_width / 2)\n",
    "                top = int(center_y - box_height / 2)\n",
    "\n",
    "                # Apply non-maximum suppression to remove overlapping boxes\n",
    "                box = [left, top, box_width, box_height]\n",
    "                \n",
    "                boxes.append(box)\n",
    "                confidences.append(float(confidence))\n",
    "    \n",
    "    # Apply non-maximum suppression to remove overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "    # Draw the final bounding boxes on the cropped image\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box\n",
    "        cv2.rectangle(crop_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Save the cropped image to a file\n",
    "    filename = f\"crop_{i}.png\"\n",
    "    cv2.imwrite(filename, crop_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bassem\n",
      "bassem\n",
      "bassem\n"
     ]
    }
   ],
   "source": [
    "image_array = []\n",
    "\n",
    "for i, contour in enumerate(contours):\n",
    "    print(\"bassem\")\n",
    "    # Load the input image\n",
    "    image = cv2.imread(f\"crop_{i}.png\")\n",
    "\n",
    "    # Get the original dimensions of the image\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Calculate the aspect ratio of the image\n",
    "    aspect_ratio = width / height\n",
    "\n",
    "    # Set the desired output size (height or width)\n",
    "    output_size = 224\n",
    "\n",
    "    # Calculate the other dimension based on the aspect ratio\n",
    "    if width > height:\n",
    "        new_width = output_size\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = output_size\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # Create a new blank image with the desired output size\n",
    "    new_image = np.zeros((output_size, output_size, 3), np.uint8)\n",
    "    new_image[:, :] = (255, 255, 255)  # fill with black pixels\n",
    "\n",
    "    # Calculate the coordinates to copy the resized image onto the new image\n",
    "    x_offset = int((output_size - new_width) / 2)\n",
    "    y_offset = int((output_size - new_height) / 2)\n",
    "\n",
    "    # Copy the resized image onto the new image with black pixels\n",
    "    new_image[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = resized_image\n",
    "\n",
    "    # Save the final image to a file\n",
    "    cv2.imwrite(f\"resize{i}.png\", new_image)\n",
    "\n",
    "    image = cv2.imread(f\"resize{i}.png\")\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "    inv = cv2.bitwise_not(thresh)\n",
    "    seg_img = cv2.imwrite(f\"seg_img{i}.png\", inv)\n",
    "    image_array.append(seg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14.6810,  6.4098,  8.2626,  ...,  0.3235,  1.4092,  4.0678])\n",
      "D021 0.9835072755813599\n",
      "D022 0.00982186570763588\n",
      "D023 0.006269935984164476\n",
      "R005 0.00012702718959189951\n",
      "T030 0.00011304713552817702\n",
      "D021\n",
      "tensor([ 9.7802,  3.1250,  6.2839,  ..., 25.4898, 15.7736, 12.7743])\n",
      "O001 0.8314805626869202\n",
      "AA012 0.055264417082071304\n",
      "N036 0.04990420117974281\n",
      "F018 0.04530208557844162\n",
      "D051 0.006555827334523201\n",
      "O001\n",
      "tensor([18.9603, 18.5760, 13.6490,  ...,  9.8656,  4.5838,  3.9390])\n",
      "D055 0.7123855352401733\n",
      "M014 0.1046474501490593\n",
      "D054 0.0649421438574791\n",
      "H007 0.017314428463578224\n",
      "AA007B 0.016710810363292694\n",
      "D055\n",
      "D021 O001 D055\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "import cv2\n",
    "\n",
    "results = \"\"\n",
    "#Basma\n",
    "for i in range(len(image_array)):\n",
    "    input_image = Image.open(f'seg_img{i}.png').convert('RGB')\n",
    "    # input_image = Image.open('C:/Users/basma/Fourth Year/Graduation_Project/SegmentedDatasetAgain/train/A040/A040_1.png').convert('RGB')\n",
    "    #Bassem\n",
    "    # input_image = Image.open('D:/Ahmed Bassem/MIU/Year 4/Graduation Project/Datasets/SegmentedDataset/SegmentedDataset/test/O001/O001_1.png').convert('RGB')\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        # transforms.Pad(50),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    # input_tensor = preprocess(Image.fromarray(i))\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    # Tensor of shape 1000, with confidence scores over Aegyptos' 1072 classes\n",
    "    print(output[0])\n",
    "    # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "    # Read the categories\n",
    "    #Basma\n",
    "    with open(\"classes_names.txt\", \"r\") as f:\n",
    "\n",
    "    #Bassem\n",
    "    # with open(\"D:/Ahmed Bassem/MIU\\Year 4/Graduation Project/classes_names.txt\", \"r\") as f:\n",
    "        categories = [s.strip() for s in f.readlines()]\n",
    "    # Show top categories per image\n",
    "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "    for i in range(top5_prob.size(0)):\n",
    "        print(categories[top5_catid[i]], top5_prob[i].item())\n",
    "\n",
    "    top1 = categories[top5_catid[0]]\n",
    "    print(top1)\n",
    "    # results = results + top1 + \" \"\n",
    "    results += \" \"+ top1\n",
    "updated_results = results.replace(\" \", \"\", 1)\n",
    "    # results.append(top1)\n",
    "print(updated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n"
     ]
    }
   ],
   "source": [
    "import matching_algorithm as ma\n",
    "print(ma.dictionary_matching(updated_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بري\n"
     ]
    }
   ],
   "source": [
    "import matching_algorithm_pronunciation as ma\n",
    "print(ma.dictionary_matching_pronunciation(updated_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans\n",
    "from googletrans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = ma.dictionary_matching(updated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يمشي\n"
     ]
    }
   ],
   "source": [
    "translator = googletrans.Translator()\n",
    "translate=translator.translate(match,dest='arabic')\n",
    "print(translate.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
