{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sigma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "#Basma\n",
    "# classifer = joblib.load(\"C:/Users/basma/Fourth Year/Graduation_Project/squeezenetfinetuned.pkl\")\n",
    "\n",
    "#Bassem\n",
    "classifer = joblib.load(\"squeezenetuntrained.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1072, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded_model = pickle.load(\"squeezenetfinetuned.pkl\")\n",
    "import sklearn \n",
    "import numpy as np\n",
    "\n",
    "model = pickle.load(open('squeezenetuntrained.pkl', 'rb'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmentation then Cropping algorithm \n",
    "import cv2\n",
    "from imutils import contours\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "# Load image, grayscale, Otsu's threshold\n",
    "image_array = []\n",
    "results = []\n",
    "# image = cv2.imread('C:/Users/basma/Fourth Year/Graduation_Project/O001 (1).png')\n",
    "# image_array.append(image)\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "# inv = cv2.bitwise_not(thresh)\n",
    "# rgb = cv2.cvtColor(inv, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "def Otsu(myImage):\n",
    "  sample_image = cv2.imread(myImage)\n",
    "  img = cv2.cvtColor(sample_image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  # plt.axis('off')\n",
    "  # plt.imshow(img)\n",
    "\n",
    "  img_gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "  thresh = threshold_otsu(img_gray)\n",
    "  img_otsu  = img_gray < thresh\n",
    "\n",
    "  # plt.imshow(img_otsu)\n",
    "\n",
    "  filtered = filter_image(img, img_otsu)\n",
    "  filteredBW=cv2.cvtColor(filtered,cv2.COLOR_RGB2GRAY)\n",
    "  return filteredBW\n",
    "\n",
    "\n",
    "\n",
    "def filter_image(image, mask):\n",
    "\n",
    "    r = image[:,:,0] * mask\n",
    "    g = image[:,:,1] * mask\n",
    "    b = image[:,:,2] * mask\n",
    "\n",
    "    return np.dstack([r,g,b])\n",
    "\n",
    "image = Otsu('C:/Users/basma/Fourth Year/Graduation_Project/D021.png')\n",
    "\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "# inv = cv2.bitwise_not(thresh)\n",
    "\n",
    "cv2.imwrite('image2.png', image)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Find contours, sort from left-to-right, then crop\n",
    "# cnts = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "# cnts, _ = contours.sort_contours(cnts, method=\"left-to-right\")\n",
    "\n",
    "# ROI_number = 0\n",
    "# for c in cnts:\n",
    "#     area = cv2.contourArea(c)\n",
    "#     if area > 50:\n",
    "#         x,y,w,h = cv2.boundingRect(c)\n",
    "#         ROI = 255 - image[y:y+h, x:x+w]\n",
    "#         img1 = cv2.copyMakeBorder(ROI, 20,20,20,20,cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "#         cv2.imwrite('ROIVVV_{}.png'.format(ROI_number), img1)\n",
    "        \n",
    "#         # image_array.append(img1)\n",
    "#         cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "#         ROI_number += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.2779,  3.9257,  7.1866,  ...,  1.0307,  0.5112,  0.8537])\n",
      "D021 0.9993090629577637\n",
      "S010 0.00026815372984856367\n",
      "S002 0.0001975266495719552\n",
      "D022 8.038392115850002e-05\n",
      "D048A 7.427437230944633e-05\n",
      "D021\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "import cv2\n",
    "\n",
    "#Basma\n",
    "input_image = Image.open('image2.png').convert('RGB')\n",
    "# input_image = Image.open('ROIVVV_0.png').convert('RGB')\n",
    "# input_image = Image.open('C:/Users/basma/Fourth Year/Graduation_Project/SegmentedDatasetAgain/train/A040/A040_1.png').convert('RGB')\n",
    "#Bassem\n",
    "# input_image = Image.open('D:/Ahmed Bassem/MIU/Year 4/Graduation Project/Datasets/SegmentedDataset/SegmentedDataset/test/O001/O001_1.png').convert('RGB')\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # transforms.Pad(50),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "# input_tensor = preprocess(Image.fromarray(i))\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Tensor of shape 1000, with confidence scores over Aegyptos' 1072 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "# Read the categories\n",
    "#Basma\n",
    "with open(\"classes_names.txt\", \"r\") as f:\n",
    "\n",
    "#Bassem\n",
    "# with open(\"D:/Ahmed Bassem/MIU\\Year 4/Graduation Project/classes_names.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())\n",
    "\n",
    "top1 = categories[top5_catid[0]]\n",
    "print(top1)\n",
    "results.append(top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\basma\\Fourth Year\\Graduation_Project\\Aegyptos backend\\Aegyptos\\predictions.ipynb Cell 5\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/basma/Fourth%20Year/Graduation_Project/Aegyptos%20backend/Aegyptos/predictions.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# im = Image.fromarray(image_array[0])\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/basma/Fourth%20Year/Graduation_Project/Aegyptos%20backend/Aegyptos/predictions.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# im.show()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/basma/Fourth%20Year/Graduation_Project/Aegyptos%20backend/Aegyptos/predictions.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m,image_array[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/basma/Fourth%20Year/Graduation_Project/Aegyptos%20backend/Aegyptos/predictions.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/basma/Fourth%20Year/Graduation_Project/Aegyptos%20backend/Aegyptos/predictions.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# im = Image.fromarray(image_array[0])\n",
    "# im.show()\n",
    "cv2.imshow(\"test\",image_array[1])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A040 0.8683706521987915\n",
      "A040A 0.08197934925556183\n",
      "A049 0.03442122042179108\n",
      "C003 0.00968263577669859\n",
      "A041 0.0015775265637785196\n",
      "A040\n"
     ]
    }
   ],
   "source": [
    "# # Read the categories\n",
    "\n",
    "# #Basma\n",
    "# # with open(\"C:/Users/basma/Fourth Year/Graduation_Project/classes_names.txt\", \"r\") as f:\n",
    "\n",
    "# #Bassem\n",
    "# with open(\"D:/Ahmed Bassem/MIU\\Year 4/Graduation Project/classes_names.txt\", \"r\") as f:\n",
    "#     categories = [s.strip() for s in f.readlines()]\n",
    "# # Show top categories per image\n",
    "# top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "# for i in range(top5_prob.size(0)):\n",
    "#     print(categories[top5_catid[i]], top5_prob[i].item())\n",
    "\n",
    "# top1 = categories[top5_catid[0]]\n",
    "# print(top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['O001'], 75, 'House'), (['O049'], 75, 'Ton,Village'), (['A001'], 50, 'Man and his occupation'), (['A040'], 50, 'King'), (['N005'], 50, 'Sun')]\n",
      "(['O001'], 75, 'House')\n",
      "temp =O004\n",
      "[(['O034 V031 D021'], 70, 'Seker or Soker'), (['Q001 O001'], 67, 'Tomb'), (['O004 V031 D021 A040'], 64, 'Soker (The god)'), (['O001'], 62, 'House'), (['V031 G001 A002'], 61, 'Think')]\n",
      "(['O034 V031 D021'], 70, 'Seker or Soker')\n",
      "temp =O004 V031\n",
      "b:-\n",
      "[(['O034 V031 D021'], 93, 'Seker or Soker'), (['O004 V031 D021 A040'], 85, 'Soker (The god)'), (['O001 D021 D054'], 71, 'walk'), (['V031 G001 A002'], 71, 'Think'), (['O001 D021 X001 N005'], 67, 'winter season')]\n",
      "(['O034 V031 D021'], 93, 'Seker or Soker')\n",
      "temp =O004 V031 D021\n",
      "b:-\n",
      "[(['O004 V031 D021 A040'], 100, 'Soker (The god)'), (['O034 V031 D021'], 79, 'Seker or Soker'), (['Q001 O004 G001 A009'], 74, 'Cash payment'), (['V031 G001 A002'], 73, 'Think'), (['W011 D021 A002'], 67, 'Silent')]\n",
      "(['O004 V031 D021 A040'], 100, 'Soker (The god)')\n",
      "b:Soker (The god)\n",
      "[(['O004 V031 D021 A040'], 88, 'Soker (The god)'), (['Q001 O004 G001 A009'], 70, 'Cash payment'), (['V031 G001 A002'], 68, 'Think'), (['O034 V031 D021'], 68, 'Seker or Soker'), (['M017 G001 V031 D058 D003'], 67, 'Mourn')]\n",
      "(['O004 V031 D021 A040'], 88, 'Soker (The god)')\n",
      "temp =O004 V031 D021 A040 O001\n",
      "[(['O001'], 100, 'House'), (['A001'], 75, 'Man and his occupation'), (['G001'], 75, 'Vulture'), (['Q001'], 75, 'Daughter, daughter in law, granddaughter'), (['Q001 O001'], 62, 'Tomb')]\n",
      "(['O001'], 100, 'House')\n",
      "[(['O001 D021 D054'], 78, 'walk'), (['O034 V031 D021'], 78, 'Seker or Soker'), (['Q001 D002 D021'], 70, 'Authority'), (['E023 O001 D036'], 70, 'Warehosue, storehouse, Labour establishment, ergastulum'), (['Q001 O001'], 67, 'Tomb')]\n",
      "(['O001 D021 D054'], 78, 'walk')\n",
      "temp =O001 D021\n",
      "b:-\n",
      "[(['O001 D021 D054'], 100, 'walk'), (['O001 D021 X001 N005'], 73, 'winter season'), (['D021 D036 N005'], 71, 'Ra or Re'), (['O034 V031 D021'], 71, 'Seker or Soker'), (['Q001 D002 D021'], 71, 'Authority')]\n",
      "(['O001 D021 D054'], 100, 'walk')\n",
      "b:walk\n",
      "--------------\n",
      "Soker (The god)\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "import matching_algorithm as ma\n",
    "\n",
    "input_array = ['O004', 'V031', 'D021', 'A040','O001', 'D021', 'D054']\n",
    "\n",
    "def find_last_true_string(input_array):\n",
    "    resArray=[]\n",
    "    last_true_string = \"\"\n",
    "    last_meaning =\"\"\n",
    "    for i,input_element in enumerate(input_array):\n",
    "        if i != 0 :\n",
    "            if iter+1 < len(input_array):\n",
    "                i = iter + 1\n",
    "            else: \n",
    "                break\n",
    "        input_str = str(input_array[i])\n",
    "        meaning,result = ma.dictionary_matching(input_str)\n",
    "        iter=i\n",
    "        # x =\"\"\n",
    "        while result:\n",
    "            if input_array[iter] != input_array[-1]:\n",
    "                temp=input_str +\" \"+ str(input_array[iter+1])\n",
    "                meaning,result = ma.dictionary_matching(temp)\n",
    "                if result:\n",
    "                    print(\"b:\" + meaning)\n",
    "                    input_str = input_str +\" \"+ str(input_array[iter+1])\n",
    "                    last_true_string = input_str\n",
    "                    last_meaning = meaning\n",
    "                    iter+=1\n",
    "            else:\n",
    "                last_true_string=input_str\n",
    "                last_meaning = meaning\n",
    "                break\n",
    "        \n",
    "    \n",
    "        resArray.append(last_meaning)\n",
    "    # resArray2,_=ma.dictionary_matching(resArray)\n",
    "    return resArray\n",
    "\n",
    "\n",
    "arr = find_last_true_string(input_array)\n",
    "print(\"--------------\")\n",
    "for i in arr:\n",
    "    print (i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'AA029' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\basma\\Fourth Year\\Graduation_Project\\Aegyptos backend\\Aegyptos\\predictions.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/basma/Fourth%20Year/Graduation_Project/Aegyptos%20backend/Aegyptos/predictions.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatching_algorithm\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mma\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/basma/Fourth%20Year/Graduation_Project/Aegyptos%20backend/Aegyptos/predictions.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(ma\u001b[39m.\u001b[39;49mdictionary_matching(top1))\n",
      "File \u001b[1;32mc:\\Users\\basma\\Fourth Year\\Graduation_Project\\Aegyptos backend\\Aegyptos\\matching_algorithm.py:68\u001b[0m, in \u001b[0;36mdictionary_matching\u001b[1;34m(top1)\u001b[0m\n\u001b[0;32m     66\u001b[0m key_list\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(str_list\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     67\u001b[0m val_list\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(str_list\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m---> 68\u001b[0m ind\u001b[39m=\u001b[39mval_list\u001b[39m.\u001b[39;49mindex(top1)\n\u001b[0;32m     69\u001b[0m key_list[ind]\n\u001b[0;32m     71\u001b[0m \u001b[39m# print(key_list[ind])\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: 'AA029' is not in list"
     ]
    }
   ],
   "source": [
    "import matching_algorithm as ma\n",
    "print(ma.dictionary_matching(top1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans\n",
    "from googletrans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = ma.dictionary_matching(top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "منزل\n"
     ]
    }
   ],
   "source": [
    "translator = googletrans.Translator()\n",
    "translate=translator.translate(match,dest='arabic')\n",
    "print(translate.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b07c767ad8f004fe0938b9527e9b3cbab4b30731c7ea224539f3f6ee22ec1af3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
